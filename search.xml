<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Auto-tuning Streamed Applications on the Intel Xeon Phi]]></title>
    <url>%2F2018%2F06%2F02%2FAuto-tuning%20Streamed%20Applications%20on%20Intel%20Xeon%20Phi%2F</url>
    <content type="text"><![CDATA[Auto-tuning Streamed Applications on the Intel Xeon PhiPublished at the conference of IPDPS18 (i.e. International Parallel &amp; Distributed Processing Symposium) ,which is a B class conference. Conference Date:2018-05-21 32nd IPDPS. Peng Zhang∗, Jianbin Fang∗, Tao Tang∗, Canqun Yang∗ Zheng Wang#. Main ideaMany-core accelerators allow software to exploit spatial and temporal sharing of computing resources to improve the overall system performance. For unlocking this performence potential : 1.divide the hardware resource to get the max overlop between host-device communication and accelerator computation; 2.match the granularity of task parallelism to the resource partition. It’s hard to find out the right resource partition and task parallelism: 1.the number of possible solutions is huge; 2.in case choosing a failure,hurting the performence seriously. In this paper, they presented an automatic approach to determine the hardware resource partition and the task granularity for any given application, targeting the Intel Xeon Phi architecture. They employed machine learning techniques to automatically learn the heuristic algorithm. 1.First learning a predictive model offline using training programs(?); 2.Then use the learned model to predict the resource partition and task granularity for any unseen programs at runtime. Environment: 23 representative(??) parallel applications A CPU-Xeon Phi mixed heterogenous many-core platform,a general purposed multi-core CPU and a 57-core Intel XeonPhi coprocessor. Knigts Corner Xeon Phi 31S1P Motivation1.Heterogeneous many-core systems are now commonplace. 2.Effectively leveraging such platforms not only enables the achievement of high performance, but also increases the energy efficiency. heterogeneous many-core design brought a problem that software developers are finding it increasingly hard to deal with the complexity of these systems. Especially the host-device communication. Heterogeneous streaming &lt;2016 IEEE International Parallel and Distributed Processing Symposium Workshops &gt; Approach Partitioning the processor cores (Main idea ) Streams means computation tasks Intention : allow independent communication and computation tasks (i.e. streams) to run concurrently on different hardware resources (overlaps the kernel execution with data movements). Representative implementations: CUDA Steams,OpenCL command queues,Intel’s hStreams program spawnning multiple-pipelines multiple-pipelines the data movement stage overlaps the kernel execution stage of another 1.While offering heterogeneous stream execution,Intel XeonPhi coprocessor provides some unique features that are currently unavailable on the GPU. Phi can specify the number of the Streams , also can explicitly map streams to different groups of cores on Xeon Phi to control the number of cores of each hardware partition.GPU can’t . So the optimizations infeasible for phi . 2.stream configuration (the number of processor core partitions the number of concurrent tasks of a streamed application) impacts the performence on phi. 3.Finding the optimum values through exhaustive search would be ineffective. A technique that automatically determines the optimal stream configuration for any streamed application in a fast manner. ContributionThis paper presents a novel runtime approach to determine the right number of partitions and tasks for heterogeneous streams, targeting the Intel Xeon Phi architecture. By employing machine learning techniques to automatically construct a predictive model to decide at runtime the optimal stream configuration for any streamed application. Their approach achieved , on average, a 1.6x (up to 5.6x) speedup, which translates to 94.5% of the performance delivered by a theoretically perfect predictor. • We present the first machine learning model for automatically determining the optimal stream configuration on Intel Xeon Phi. Showing how machine learning can be used to address the challenging problem of tuning stream configurations; • We develop a fully automatic approach for feature selection and training data generation;• We show that our approach delivers constantly better performance over the single-streamed execution across programs and inputs;• Our approach is immediately deployable and does not require any modification to the application source code. BackgroundHeterogeneous Streams: exploiting the temporal and spatial sharing of the computing resources Temporal Sharing. one can overlap some of communication and computation to exploit pipeline parallelism to improve performance divide an application into independent tasks so that they can run in a pipeline fashion. Spatial Sharing. partition the computing units into multiple groups to concurrently execute multiple tasks. The key to spatial sharing is to determine the right number of partitions. Problem Scope improve the performance of a data parallel application by exploiting spatial and temporal sharing of heterogeneous streams partitions (partitions should be used to group the cores ) tasks(data parallel tasks) hStreams Our predictive model determines the #partitions and the #tasks before invoking the hStreams initialization routine, hStreams_app_init(). Motivating Examples As can be seen from the diagrams, the search space of stream configuration is huge but good configurations are sparse. The performance varies significantly over stream configurations (#partitions, #tasks). As can be seen from the figure, the best stream configuration can vary across inputs for the same benchmark. Lesson Learned. Choosing the stream configuration has a great impact on the resultant performance and the best configuration must be determined on a per-program and per-dataset basis exhaustive search Online search algorithms: simulated annealing Classical hand-written heuristics An alternate approach, and the one we chose to use, is to use machine learning to automatically construct a predictive model directly predict the best configuration, providing minimal runtime, and having little development overhead when targeting new architectures. To determine the best streaming configuration,steps： 1.We use a set of information or features to capture the characteristics of the program. 2.We develop a LLVM compiler pass to extract static code features at compile time, and a low-overhead profiling pass to collect runtime information at execution time. 3.At runtime, a predictive model (that is trained offline) takes in the feature values and predicts the optimal stream configuration WorkPREDICTIVE MODELING1.trained off-line 2.using code and dynamic runtime features of the program to predicts the best configuration of new program . Our model is a Support Vector Machine (SVM) with a quadratic function kernel. evaluated a number of alterna- tive modeling techniques including regression, K-Nearest neighbour (KNN), decision trees, and the artificial neural network (ANN). SVM gives the best performance and can model both linear and non-linear problems. Training the Predictor 1) Generating Training Data We use cross validation by excluding the testing benchmarks from the training dataset. the upper and lower confidence bounds is smaller than 5% under a 95% confidence interval setting 15 programs We exhaustively execute each training program across all of our considered stream configuration and record the performance of each.e #partitions ranging from 1 to 224 and the #tasks ranging from 1 to 256 2. we record the best performing configuration for each program and dataset, keeping a label of each. Finally, we extract the values of our selected set of features from each program and dataset Data Labeling. (a) the common best configurations (b) whether the samples are from the same program (c) whether they are with the same dataset. Building The Model The algorithm finds a correlation between the feature values and the optimal stream configuration weights of the model are determined from the training data. We use the parameter tuning tool provided by libSVM to determine the kernel parameters. Parameter search is performed on the training dataset using cross-validation. Features Our predictive models are based exclusively on code and dynamic features of the target programs 1)selection: We considered 38 candidate raw features in this work. Then reduced it to 10 G. Fursin et al., “Mi lepost gcc: machine learning based research compiler,” Z. Wang et al., “Automatic and portable mapping of data parallel programs to opencl for gpu-based heterogeneous systems,” 2) Feature Scaling: scaled the value for each of our features between the range of 0 and 1. 3) Feature Importance: Varimax rotation principal component analysis (PCA). B. F. Manly, Multivariate statistical methods: a primer. CRC Press, 2004. Runtime Deployment When an application is launched, we will first extract the feature values of the program. Code features (such as loop count) are extracted from the program source. Dynamic features (such as branch miss) are extracted by profiling the program without partitioning for several microseconds. After feature collection, we feed the feature values to the offline trained model which outputs a label indicating the stream configuration to use for the target program. Adapt to Changing Program Phases. across kernels phase changes within a kernel Experimental SetupPlatform. Our evaluation platform is an Intel Xeon server with an Intel dual-socket 8-core Xeon CPU @ 2.6 Ghz (16 cores in total) and an Intel Xeon 31SP Phi accelerator (57 cores). The host CPUs and the accelerator are connected through PCIe. The host environment runs Redhat Linux v7.0 (with kernel v.3.10). The coprocessor environment runs a customized uOS (v2.6.38.8). We use Intel’s MPSS (v3.6) to communicate between the host and the coprocessor and Intel’s hStreams library (v3.6). Benchmarks. 23 programs 25 different datasets, some 10 datasets Competitive Approaches Liu et al: In [12], Liu et al. use linear regression models to search for the optimal number of tasks for GPU programs [12]. Werkhoven et al.: The work presented by Werkhoven et al. models the performance of data transfers between the CPU and the GPU. Evaluation Methodology AMD and NVIDIA SDK suites, Parboil suite. Experimental ResultsThis demonstrates the portability of our approach across benchmarks. 94.5% 97.8% a speedup of over 2× reduce the kernel execution time for these applications Speedup Distribution the streaming speedups of some applications are sensitive to the input datasets while that of others are not. Correlation Analysis the computation-communication ratio and the speedup) have a strong linear correlation The performance improvement of our approach comes from two factors. First, by predicting the right processor partition, our approach allows effective overlapping of the host-device communication and computation. Second, by matching task parallelism to the resource partition, our approach can reduce the overhead of thread management. Compare to Fixed Stream Configuration This experiment confirms that a fixed configuration failsto deliver improved performance across applications and datasets, and selecting a right stream configuration on a per program, per dataset basis is thus required. Compare to Alternated Models Thus, our approach delivers consistently better performance compared with the alternative models. Model Analysis our label merging algorithm can lead to a better predictive performance by better balance training samples per stream configuration. This is because the kernel function we used can model both linear and non-linear relation between the features and the desired labels , it predicts the best stream configuration more accurate than other alternative models. CONCLUSIONpresented an automatic approach to exploit heterogenous streams on heterogenous many-core architectures. a machine learning based approach that predicts the optimal processor core partition and parallel task granularity. Experimental results show that our approach delivers, on average, a 1.6x speedup over a single-stream execution. This translates to 94.5% of the performance given by an ideal predictor.]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Hexo的Next主题美化自己的Blog]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%88%A9%E7%94%A8Hexo%E7%9A%84Next%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96%E8%87%AA%E5%B7%B1%E7%9A%84Blog%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>BLOG</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在github上搭自己的小窝]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%9C%A8github%E4%B8%8A%E6%90%AD%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E7%AA%9D%2F</url>
    <content type="text"><![CDATA[1 缘由想起来在github上搭自己的小窝，源自于一次课堂作业。 用的sublime text3来写的代码，因为用了自动模板工具sublime Tmpl来新建的python文件，最前端的注释里有link，想了想，开始上学这半年以来，自己的笔记也是写了不少，以前的话都是用Wiz在记，有点乱，平常都是全局搜索来找，也很久没有整理过了。因此想借着这个机会把笔记梳理一遍，POST到网上来，也是对自己学习的一个整理和回顾。 2 博客选窝前阵子交爬虫作业的时候在知乎上找到一个比较好玩的知乎er小歪,也跟着到他的BLOG上逛了逛，很简约，干货也不少，并且页面很漂亮，尤其是看板娘很萌。并且他给了一篇免费搭建个人博客的文章,因此也萌生了一把就把博客放在github里面去得了的想法。 3 搭建博客3.1 工具环境系统：windows 7 64位，编辑器：sublime text3，控制台：gitbash 搭建博客主要使用hexo+Github markdown工具：typora 美化工具：live2D 3.2 大体回顾按照小歪的教程，搭建简单的测试往网站很快，我用了大概一个小时，搭建出了大概的框架，然后美化用了1天半(｀・ω・´)。中间同样遇到了很多困难，踩了不少坑。在国内当程序员挺大的好处，就是把问题直接丢给度娘，大概会有上百页跟你遇到类似问题的人，网上能找到各种教程和回答，解决问题效率很高。 3.3 开始了3.3.1 搭环境之Hexo)Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown解析文章，在几秒内，即可利用靓丽的主题生成静态网页。我们所需要就是把Hexo下载到本地，在标题的链接里面可以打开Hexo官网的中文说明文档。按照如下步骤进行就可以了，很简单。 首先要安装两个程序（我只举例了我在win7 X64环境下的安装策略 Linux和Mac用户还是参见Hexo） Node.js 我用的是8.11.2LTS git 我用的是2.17.1 的安装版本 这两个下载之后直接进行安装，除了git需要勾选请勾选Add to PATH选项，其余默认设置即可。Git使用教程可以用网络上廖雪峰老师的教程。 安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，初次使用输入Github上的名称和邮箱 接下来只需要使用 npm（node.js的包管理工具） 即可完成 Hexo 的安装。 1$ npm install -g hexo-cli 我是在c盘手工建了一个test文件夹，在文件夹里，右键打开git bash，输入 123$ hexo init Blog ##在test文件夹里建立Blog文件夹，hexo会自动建立这个文件夹，以后博客就在这里写了$ cd &lt;folder&gt; ##切换到Blog文件夹$ npm install ##安装npm到路径 会有提示INFO Start blogging with Hexo!新建完成后，指定文件夹的目录如下： 12345678.├── _config.yml ##建站信息，后面美化还要用├── package.json ##用用程序的信息默认安装了EJS, Stylus 和 Markdown renderer ├── scaffolds ##模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件，后面美化时候用这个在模板里添加标签、分类和是否开启版权。├── source ##存放用户资源，上传到github时候开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。| ├── _drafts ## 隐藏| └── _posts└── themes ## 默认的模板主题是landscape，挺丑的一个，我是拒绝的 在初始化Hexo之后source目录下自带有一篇hello world.md的markdown文件，可以直接建站。 1234$ hexo generate ##生成网站，把新添加的markdown文件渲染后放到public文件夹中$ hexo server -p 4001 ##启动本地服务器，默认的4000端口很有可能打不开，所以用4001端口INFO Start processingINFO Hexo is running at http://localhost:4001/. Press Ctrl+C to stop。 这时候在浏览器输入 http://localhost:4001/就可以看见网页和模板了 整个Hexo的建站过程是利用theme里面，你采用的模板来进行的网站生成，并且把source文件夹中的Markdown 和 HTML 文件解析，并放到 public 文件夹，而其他文件会被拷贝过去。这样就在blog的public文件夹中生成了一个静态页面网站。 接下来我们需要把这个网站放到github中去，以便于我们输入网址就可以访问。 3.3.2 搭环境之Github在Blog文件夹中，右键gitbash 或者是在刚才的bash中ctrl+c停止网站运行。之后就将gitbash和github相连，我这里假定 github的邮箱是xxx@qq.com，用户名是XaX。 1ssh-keygen -t rsa -C "xxx@qq.com" 会有提示生成了公用钥匙 1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 就是生成访问xxx@qq.com邮箱注册的github的ssh钥匙，分别有公用钥匙public key（这个要放到github网站上去），另一外个就是你自己的私钥，这个可以加密，也可以不加，加密的话，就是每次从gitbash中部署网站到github都需要输入这个密码。 打开这个.pub的公用钥匙，用sublime text打开或者用记事本打开，得到一串ssh-rsa 开头的字符，复制下来。 打开浏览器 登录你的Github https://github.com/settings/keys，点击右上角的New SSH key， 然后输入 title：blog》Key： 把刚才复制的粘贴进去》Add SSH key 这个时候我们就可以用github来远程ssh登录Github了，我们需要先在浏览器里，https://github.com/new新建一个和自己的用户名XaX一样但是加上.github.io的Repository 仓库，即XaX.github.io，一定要建立，不然会莫名其妙踩坑。 3.3.3 博客配置(站点设置)在Blog目录下，用sublime打开_config.yml（站点配置）文件，修改参数信息 。这个后面在美化里还要讲，因为涉及到主页live2d萌妹子的问题。 特别提醒，在每个参数的：后都要加一个空格，显示效果如下： 就是左边的那些，这里还有个坑，初始配置默认用的是landscape模板 language： zh-CN而后面我们会讲到，我用的是Next模板。要把这一栏改成zh-Hans 配置部署（我的是sheldon-pan，修改成自己的） 1234deploy: type: git repo: https://github.com/sheldon-pan/sheldon-pan.github.io.git branch: master 3.4 新建文章在刚才的gitbash中，应该l路径还是在blog目录下输入 12$ hexo new &quot;文章名字_引号要带上&quot;INFO Created: c:\test\Blog\source\_posts\文章名字_引号要带上.md 这时候在Blog 的sources/_post文件夹下面会新建一个这样的md文件 我们用typora打开就可以直接做编辑了，typora的下载https://www.typora.io/。挺好用的一个可视化markdown编辑器，所见即所得，不用自己输入md语法，适合初学者。 12345678---title: Ubuntu环境配置date: 2018-06-02 09:06:26tags: Ubuntucategories: 作业copyright: True---这是一篇测试文章，欢迎关注作者博客https://sheldon-pan.github.io/ 其中的tags（标签）、 categories（分类）、copyright（版权）功能放到美化一文了，详见后面一篇。 保存，然后执行 12345678910111213C:\test\Blog$ hexo clean #清除掉Public文件夹INFO Deleted database.INFO Deleted public folder.$ hexo generate #重新生成网站文件INFO Start processingINFO Files loaded in 1.56 sINFO 144 files generated in 3.33 s$ hexo server -p 4001 #在端口4001搭建站点INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 这个时候，打开http://localhost:4001/，发现刚才的文章已经成功了 。 3.5 发布网站最后一步，发布到Github，在https://sheldon-pan.github.io/，可以访问 12345$ hexo deployINFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...#省略 其中会跳出Github登录，直接登录，如果没有问题输入sheldon-pan（换成你的）.github.io/ 就可以看到已经发布了 4 总结以后更新博文的步骤： 1、hexo new 创建文章 2、用typora编辑文章 3、部署（所有打开CMD都是在blog目录下） 1234hexo clean #清除public缓存 网页正常情况下就不用执行本步骤hexo generate #生成hexo server -p 4001 #启动服务预览，非必要，可本地浏览网页hexo deploy #部署发布 简写： hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo g == hexo generate#生成 hexo s -p 4001 == hexo server #启动服务预览 hexo d == hexo deploy#部署 到这里已经完成了博客的基础搭建，接下来就是博客的美化工作，我使用的是Next主题，它的介绍和美化会用到这篇文章。 主题介绍http://theme-next.iissnan.com/ Next主题美化https://www.jianshu.com/p/f054333ac9e6]]></content>
      <categories>
        <category>BLOG</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu环境配置]]></title>
    <url>%2F2018%2F06%2F02%2Fubuntu%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1 原由由于高级并行程序设计这门课程需要用到Linux环境 ，手头上的Ubuntu版本比较老了14.04，在不用sniper来测功耗之后，也没有停留在14的必要了。所以趁着这次机会升级下系统，并且把编程环境重新配置一下。 2 过程2.1 环境硬件：联想T430s，i5-3320M(2.6GHz)双核四线程处理器，12GB内存，集成显卡，SSD1：Intel 520s 180GB（SATA3），SSD2：建兴 LMT-32GB（MSATA）； 软件： 主系统Win7（SSD1），副系统Ubuntu14.04（SSD2），两个系统双硬盘，启动信息写在各自硬盘上，在启动的时候不关联，选择不同的硬盘进入不同的系统，最大程度上避免重装系统造成的影响； 工具：16GB的U盘，16.04的ISO，Rufus2.18。 2.2 分区可以在WIn下分区，也可以在Ubuntu安装的时候再进行，这次我们采用的是安装时候进行。一般意义需要分出4个分区，分别用于Ubuntu系统分区/（安装系统和软件）、交换分区Swap （即虚拟内存分区，内存大可以不分配）、引导分区/boot （即逻辑引导分区，本次不需要，boot分区主要为多系统引导，LVM、Raid，加密以及系统崩溃厚的抢救恢复，本次安装不进行分区划分）、文档分区/home（个人文件分区，类似于Win下“我的文档”，本次不区分）。 小硬盘，虚拟机或者是初学者的话，建议就分出系统分区即可，进阶的时候或者在服务器上可以把home、 /boot、swap和/Var（主要放置系统执行过程中经常变化的文件，例如缓存（cache）或者是随时更改的登录文件log file。 如果计算机主要是提供网页服务，或者是数据库，那/var会大量增加，最好能够把/var额外分割出来）。 2.3 制作启动U盘在Win下利用Rufus选中U盘盘符之后，勾选“创建一个启动盘使用”选项，并点击后面的光驱小图标选择ISO 镜像文件，没特殊需求的话，取消检查设备坏块、勾选快速格式化，点击开始进行启动盘的制作（下图）。 2.4 安装重启之后通过U盘引导，进入live系统, 双击桌面“安装Ubuntu”开始安装，选择语言后进入“安装类型”选项，选择“其他选项”,进行自主分区，本次将划分出的分区全部分配给/主分区，文件格式采用ext4。 在“启动引导器的设备”下拉菜单中，本次不通过boot引导，选择硬盘根目录，即/dev/sdb。连续下一步，选择键盘、设置用户名密码，等待全部安装完毕。 2.5 配置环境2.5.1 更换更新源1sudo vim /etc/apt/sources.list 在Vim中dd100，删除掉原先的更新源，i进入Insert模式，将代码粘贴进去，Esc退出Insert模式，：进入命令行，输入wq 回车 写入并退出。 12345678910111213# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse 如果装完之后发现Ubuntu时差8小时 1sudo timedatectl set-local-rtc 1 2.5.2 删除无用软件、更新123456789101112#libreoffice 不好用杀之，后续安装wps凑合用sudo apt-get remove libreoffice-common #Amazon链接好烦人，杀之sudo apt-get remove unity-webapps-common #还有一堆 杀杀杀sudo apt-get remove thunderbird totem rhythmbox empathy brasero simple-scan gnome-mahjongg aisleriot gnome-mines cheese transmission-common gnome-orca webbrowser-app gnome-sudoku landscape-client-ui-install onboard deja-dup #干掉这么多，自动清理下sudo apt-get autoremove#更新源sudo apt upgrade#升级软件sudo apt-get update 安装编译器，和常用依赖库，16默认的gcc g++似乎都是5.4 sudo apt-get install build-essential 2.5.3 软件安装（1）搜狗输入法 搜狗拼音官方直接下载 123sudo apt-get install -fsudo dpkg -i sogou_pinyin_linux_1.0.0.0021_i386.debreboot （2）PDF阅读器（Foxit） 12tar -zxvf FoxitReader2.4.1.0609_Server_x64_enu_Setup.run.tar.gz./FoxitReader.enu.setup.2.4.1.0609\(r08f07f8\).x64.run （3）Pycharm（16.04内置了python2和python3，配置的时候可以直接用） 123sudo add-apt-repository ppa:viktor-krivak/pycharmsudo apt updatesudo apt install pycharm （4）Vim（这个还不太会用，常用subl3） 1sudo apt-get install vim （5）Chrome（神器，几乎所有常用插件能同步过来） 1234#能在线安装就不下载包离线安装wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add - sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" &gt;&gt; /etc/apt/sources.list.d/google.list'suo apt-get update &amp;&amp; sudo apt-get install google-chrome-stable （6）WPS（linux下能用的office类） 1sudo apt-get install wps-office （7）Sublime Text3（非常好用的编辑器，插件丰富，我主要拿来win10下+WSL用，插件配置下一篇再写） 123sudo add-apt-repository ppa:webupd8team/sublime-text-3 sudo apt-get update sudo apt-get install sublime-text （8）git（程序员专用，git的设置在网站搭建那篇文章里再讲） sudo apt-get install git （9）MPV（简单的影音播放器） apt-get install mpv （10）Shutter截图工具（做实验，截图作报告要用） 123sudo add-apt-repository ppa:shutter/ppa sudo apt-get update sudo apt-get install Shutter （11）Microsoft Windows Fonts微软字体库 Microsoft Windows Fonts微软字体库包含了Times_New_Roman和Courier_New等字体。安装 1sudo apt-get install ttf-mscorefonts-installer1 然后会提示微软的用户协议向导 单击“确定OK”继续,单击“YES是”接受微软协议 在安装字体后，我们需要使用命令来更新字体缓存： 1sudo fc-cache -f -v （12）为知笔记（唯一可以在linux，mac，win下同步的笔记工具，只能VIP年费60） 123sudo add-apt-repository ppa:wiznote-team sudo apt-get update sudo apt-get install wiznote （13）网易云音乐 下载地址 http://music.163.com/#/download 下载后deb包直接安装 （14）有道词典 划词翻译，在inux版下能用的貌似只有着一个 下载地址 http://cidian.youdao.com/multi.html （15）rarlinux（解压缩软件） 官方网址：http://www.rarlab.com/下载对应的源码包也可以直接用wget获取 1wget http://www.rarlab.com/rar/rarlinux-5.2.1.tar.gz1 编译安装 1234tar -zxvf rarlinux-5.2.1.tar.gzcd rarlinuxmakemake install （16）MPICH（吃饭的家伙，研究生生涯就指着这个活了） 在Ubuntu下安装MPI，参考了直接apt-get安装 和tar.gz源码安装 两种方式。网上了解到apt-get安装具有一些问题，于是我在MPI网站上下载了源码包mpich-3.2.1.tar.gz，然后解压。 sudo tar -zxvf mpich-3.2.1.tar.gz 解压完后,进行配置(我的配置安装位置为 /home/MPICH2) ./configure -prefix= /home/MPICH2 结果报错如下： “No Fortran 77 compiler found. If you don’t need to build any Fortran programs, you can disable Fortran support using –disable-f77 and –disable-fc. If you do want to build Fortran programs, you need to install a Fortran compiler such as gfortran or ifort before you can proceed.” 安装gfortran： sudo apt-get install gfortran 问题解决 配置完成后，执行编译和安装工作 sudo make &amp;&amp; sudo make install 1 安装完成后，添加环境变量 sudo vim ~/.bashrc 在最后添上下面几句话 123export MPI_ROOT=/home/MPICH2export PATH=$MPI_ROOT/bin:$PATHexport MANPATH=$MPI_ROOT/man:$MANPATH 这里有个很着重的地方，因为我在Ubuntu下采用的是zsh终端./bashrc 是写给bash 看的，而我的用的是zsh 所以要配置./zshrc文件一定要修改对，并且还要source 一下才能正常使用 2 安装完成后，添加环境变量 sudo gedit ~/.zshrc 在最后添上下面几句话 123export MPI_ROOT=/home/MPICH2export PATH=$MPI_ROOT/bin:$PATHexport MANPATH=$MPI_ROOT/man:$MANPATH source ~/.zshrc 这样就可以在zsh下使用mpirun 和mpicc mpicxx了。 MPI的具体测试和使用，也会再开一篇。 3 结束语磕磕绊绊算是把第一篇博客写完了的，最近考试完计划是把MPICH、sublime text、zsh、win10下的WSL、git.io博客的搭建还有天河超算节点的笔记都再写成博客，下一篇先写博客搭建和美化好了。]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
</search>
